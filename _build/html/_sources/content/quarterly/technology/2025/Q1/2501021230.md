# 2020s 与 1990s 的异同 - 1

在回看Apple、Microsoft、Nvidia、Google、Amazon的发展史和股价波动历史时，会发现这几家公司都是在1990s开始发迹并崭露头角。除去在彼时已经是头部企业的Microsoft和尚未IPO的Google外，只要不是在1990s末和2000s的科网泡沫最高区间入场，以Apple、Nvidia、Amazon和一众明星软硬件科技公司的投资回报率都是成百上千倍。

站在2025的起点，2020s已经过去接近一半，AI大模型的广泛应用已经有1990s互联网浪潮席卷各行各业并带来生产效率大幅提高的雏形。如果以史为鉴，展望未来技术的主要发展方向和投资方向，就必须抓住核心主线。身为计算机领域的从业人士，站在众多巨人的肩膀上包括LLM模型的辅助，我认为这条主线就是计算机领域最核心的抽象技术，从底层向顶层逐层更新迭代并完善。

计算机领域的抽象技术是指从最底层的纯硬件设备到最顶层的终端用户接触到的SaaS纯软件，中间跨越操作系统层、虚拟化层、浏览器层、云计算层，每一层都留出一个接口供其他层级调用而无需详细了解该层的具体实现细节，这大幅提高了各层级的开发效率并降低了开发门槛。

促使我以强烈动力写下本文的原因是，在与ChatGPT讨论Broadcom提出未来使用ASIC部分替代Nvidia的GPU时，ChatGPT提到GPU和ASIC的最核心区别是前者在深度学习训练模型时具备通用性而后者不具备。突然我就灵感涌现，想到1990s的GPU相比于CPU，CPU被认为是通用处理器，而GPU则只能在图形领域特别是游戏领域发挥用处，我猜想是否现在被认为只能在特定领域的ASIC在未来也会像GPU的故事一样由点到面。

要回答这个问题或找到一些线索必须从第一性原理出发，考虑2个问题：一是ASIC和GPU从底层原理考虑各自的优劣是什么？二是为什么Nvidia的GPU可以从专有领域的应用扩展到其他领域？

问题一，ChatGPT给出的核心点是，GPU具有通用性，因此在实验阶段和量产前中期的开发成本和资金成本都显著低于ASIC，切换不同算法的成本也远低于ASIC；ASIC在已经确定算法和有针对性优化领域的效率要远高于GPU，并且使用和维护成本也是远低于GPU，因此大规模量产的产品使用ASIC的性价比非常高。

问题二，ChatGPT给出的核心观点是，正是在2007年开始Nvidia对CUDA生态的构建，使得对GPU的开发门槛显著降低，CUDA生态将大部分硬件层的技术细节封装成各种软件调用接口，成为了一个抽象层，这也使得各领域的研究人员可以使用GPU其实验进行验证和小规模量产。

由此可以得出一个预测，未来训练AI大模型的核心硬件按产品开发周期的先后顺序应该就是，实验阶段和小规模量产验证阶段使用GPU，大规模量产阶段使用ASIC。目前Google、Amazon、Meta等公司已经开始与Broadcom等公司合作开发专有领域的ASIC，说明这条道路肯定不会是歧路，但是在2020s中期就介入恐怕有些为时尚早。1990s的Nvidia在发展十多年后才开发出CUDA生态，现在ASIC迫切需要一些统一的编程框架来降低开发门槛和开发成本。

有了对最底层抽象层即硬件层面ASIC的展望，就要继续向上探索，AI全面应用时代的操作系统层会是什么？操作系统层的核心是对硬件与软件的有机融合，因此多模态大模型的定位非常符合这一抽象层，并且从商业层面观察，市面上广泛应用的操作系统屈指可数，而现在市场对开发多模态大模型也有一个清晰的定位，就是“超级吞金兽”，那这注定也只能是少数玩家的游戏，这和操作系统的商业化也非常契合。

继续向上，对浏览器层的猜想其实是最没有头绪的，这也证明在技术领域，“小步迭代”的策略是相对可靠的。其实现在AI大模型的硬件层和操作系统层需要完善的空间仍然很大，连下层技术都还未明确，那对上层技术的预测只能是空中楼阁。

对于应用层其实反倒更好预测，就是将一切目前人工可以做到的全部使用AI大模型和其支持的硬件机器人。尤其是硬件机器人，理论已经证实端到端模型的调用速度将会远快于对云端大模型调用，因此将ASIC部署在大规模量产的拥有端到端大模型的机器人是可以预见的。这些机器人也将使用异步架构，CPU+GPU+ASIC并存。

总结，2020s中期的技术仍处于底层硬件层和操作系统层的发展早期，已经可以将目光聚焦在更远处的上层抽象技术，但是这不代表要现在all in，需要等到下层抽象层的技术方向明确定下后再“小步迭代”介入。毕竟，“提前三年是天才，提前三十年是疯子”。

---

*2025.1.2*